# Chanel Products Import Guide

Complete guide for fetching full product details from ERPNext and importing 112 Chanel products into the `scraped_products` table.

## Quick Start

```bash
# 1. Test ERPNext connection
npm run test:erpnext

# 2. Fetch full product details from ERPNext
npm run fetch:chanel

# 3. Import the generated JSON to Supabase
# (See "Import to Supabase" section below)
```

## Files Created

### Scripts
- `scripts/fetch-chanel-from-erpnext.js` - Main script to fetch from ERPNext
- `scripts/fetch-chanel-from-erpnext.ts` - TypeScript version for reference
- `scripts/test-erpnext-connection.js` - Connection validation script
- `scripts/README-CHANEL-FETCH.md` - Detailed documentation

### Data Files
- `product_that_contains_chanel_as_their_item_name_filtered.json` - Input (112 products)
- `chanel_products_for_import.json` - Output (generated by script)

### Package Scripts
```json
{
  "test:erpnext": "node scripts/test-erpnext-connection.js",
  "fetch:chanel": "node scripts/fetch-chanel-from-erpnext.js"
}
```

## How It Works

### 1. Input Data
The script reads from `product_that_contains_chanel_as_their_item_name_filtered.json` which contains:
- Item Code (ERPNext identifier)
- Item Name
- Description (HTML)
- Product URL
- Vendor name
- Default Unit of Measure

### 2. ERPNext API Calls
For each of the 112 products, the script:

1. **Fetch by Item Code** (`GET /api/resource/Item/{itemCode}`)
   - Retrieves full product details including:
     - Name, description, images
     - Price (standard_rate)
     - Weight, height, width, length
     - Category (item_group)
     - Brand, supplier information

2. **Fallback: Search by URL** (`POST /api/method/frappe.desk.reportview.get`)
   - If item code not found, searches by Product URL
   - Uses custom_product_url field in Item Supplier table

### 3. Data Mapping
Maps ERPNext data to `scraped_products` table schema:

| scraped_products Field | Source |
|------------------------|--------|
| `id` | Generated: `{vendor}-{item_code}` |
| `vendor` | Vendor (Supplier Items) |
| `name` | ERPNext: `item_name` |
| `price` | ERPNext: `standard_rate` |
| `original_price` | ERPNext: `standard_rate` |
| `weight` | ERPNext: `weight_per_unit` |
| `description` | ERPNext: `web_long_description` or `description` |
| `category` | ERPNext: `item_group` |
| `stock_status` | Default: `"in_stock"` |
| `images` | ERPNext: `images[]` → `image_path` |
| `main_image` | ERPNext: `image` |
| `product_id` | Item Code |
| `url` | Product URL (Supplier Items) |
| `height` | ERPNext: `custom_height` |
| `width` | ERPNext: `custom_width` |
| `length` | ERPNext: `custom_length` |
| `volumetric_weight` | Calculated: `(H × W × L) / 5000` |
| `status` | Default: `"pending"` |
| `timestamp` | Current timestamp |

### 4. Output
Generates `chanel_products_for_import.json` with 112 products ready for import.

## Import to Supabase

### Option 1: Bulk Insert via SQL (Recommended)

Create a SQL migration file:

```sql
-- Import Chanel products
INSERT INTO scraped_products (
  id, vendor, name, price, original_price, weight,
  description, category, stock_status, images, main_image,
  product_id, timestamp, url, status, height, width, length, volumetric_weight
)
SELECT
  (data->>'id')::text,
  (data->>'vendor')::text,
  (data->>'name')::text,
  (data->>'price')::numeric,
  (data->>'original_price')::numeric,
  (data->>'weight')::numeric,
  (data->>'description')::text,
  (data->>'category')::text,
  (data->>'stock_status')::text,
  (data->'images')::jsonb,
  (data->>'main_image')::text,
  (data->>'product_id')::text,
  (data->>'timestamp')::timestamptz,
  (data->>'url')::text,
  (data->>'status')::text,
  (data->>'height')::numeric,
  (data->>'width')::numeric,
  (data->>'length')::numeric,
  (data->>'volumetric_weight')::numeric
FROM jsonb_array_elements('[
  -- Paste JSON array from chanel_products_for_import.json here
]'::jsonb) AS data;
```

### Option 2: Using Supabase Client (TypeScript)

```typescript
import { supabase } from './lib/supabase/client';
import chanelProducts from './chanel_products_for_import.json';

async function importChanelProducts() {
  const { data, error } = await supabase
    .from('scraped_products')
    .insert(chanelProducts);

  if (error) {
    console.error('Import failed:', error);
  } else {
    console.log(`Successfully imported ${data?.length} products`);
  }
}
```

### Option 3: Using Supabase Edge Function

Create a Supabase Edge Function to handle large batch imports:

```typescript
import { serve } from 'https://deno.land/std@0.168.0/http/server.ts';
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

serve(async (req) => {
  const products = await req.json();

  const supabase = createClient(
    Deno.env.get('SUPABASE_URL')!,
    Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
  );

  const { data, error } = await supabase
    .from('scraped_products')
    .insert(products);

  return new Response(JSON.stringify({ data, error }), {
    headers: { 'Content-Type': 'application/json' },
  });
});
```

## Post-Import Steps

After importing the 112 Chanel products:

### 1. Verify Import
```sql
SELECT COUNT(*) FROM scraped_products WHERE vendor IN ('John Lewis', 'Harrods');
```

### 2. Create Pending Products
Create corresponding entries in `pending_products` table:

```sql
INSERT INTO pending_products (
  scraped_product_id,
  url,
  vendor,
  category_status,
  weight_and_dimension_status,
  seo_status,
  copyright_status
)
SELECT
  id,
  url,
  vendor,
  'pending',
  'pending',
  'pending',
  'pending'
FROM scraped_products
WHERE vendor IN ('John Lewis', 'Harrods')
AND product_id LIKE '%CHANEL%' OR name LIKE '%CHANEL%';
```

### 3. Run AI Agents
Process the products through the AI agent pipeline:

1. **Category Agent**: Categorize products
2. **Weight/Dimension Agent**: Validate and calculate dimensions
3. **SEO Agent**: Generate meta titles and descriptions
4. **Copyright Agent**: Create vendor-neutral content

### 4. Monitor Progress
Check the agent processing dashboard to monitor progress and view statistics.

### 5. Push to ERPNext
Once all agents complete, push enriched products back to ERPNext with:
- Validated categories
- Calculated dimensions and volumetric weight
- SEO-optimized titles and descriptions
- Copyright-compliant content

## Troubleshooting

### Issue: Products missing in ERPNext
**Symptom**: Script shows "⚠ Mapped {itemCode} (ERPNext data not found)"

**Solution**:
- These products will use data from the filtered JSON
- ERPNext fields will be `null`
- You can manually add missing data later

### Issue: Duplicate key errors on import
**Symptom**: `duplicate key value violates unique constraint "scraped_products_pkey"`

**Solution**:
- Check if products already exist: `SELECT id FROM scraped_products WHERE id LIKE 'john-lewis-%' OR id LIKE 'harrods-%'`
- Use `ON CONFLICT (id) DO UPDATE` clause:
```sql
INSERT INTO scraped_products (...) VALUES (...)
ON CONFLICT (id) DO UPDATE SET
  name = EXCLUDED.name,
  price = EXCLUDED.price,
  ...
```

### Issue: JSON parsing errors
**Symptom**: "Unexpected token" or "Invalid JSON"

**Solution**:
- Validate JSON: `cat chanel_products_for_import.json | jq .`
- Check for trailing commas, unescaped quotes
- Ensure UTF-8 encoding

### Issue: Missing environment variables
**Symptom**: "ERPNEXT_AUTH_TOKEN environment variable not set"

**Solution**:
- Check `.env` file exists
- Verify `VITE_ERPNEXT_TOKEN` or `ERPNEXT_AUTH_TOKEN` is set
- Run `npm run test:erpnext` to verify connection

## Performance Notes

- **Processing Time**: ~6-7 minutes for 112 products
- **API Rate Limit**: 500ms delay between batches (10 products/batch)
- **Memory Usage**: Minimal (~50MB for full dataset)
- **Network**: Requires stable internet connection to ERPNext

## Data Quality

### Expected Data Completeness

| Field | Availability |
|-------|-------------|
| Name | 100% (from ERPNext or filtered JSON) |
| Vendor | 100% (from filtered JSON) |
| Product URL | ~80% (some products lack URLs) |
| Price | ~60% (depends on ERPNext data) |
| Weight | ~40% (depends on ERPNext data) |
| Dimensions (H/W/L) | ~30% (depends on ERPNext data) |
| Images | ~50% (depends on ERPNext data) |
| Category | ~70% (depends on ERPNext data) |

### Data Validation

After import, run validation queries:

```sql
-- Products without price
SELECT COUNT(*) FROM scraped_products
WHERE vendor IN ('John Lewis', 'Harrods')
AND price IS NULL;

-- Products without dimensions
SELECT COUNT(*) FROM scraped_products
WHERE vendor IN ('John Lewis', 'Harrods')
AND (height IS NULL OR width IS NULL OR length IS NULL);

-- Products without images
SELECT COUNT(*) FROM scraped_products
WHERE vendor IN ('John Lewis', 'Harrods')
AND (images IS NULL OR jsonb_array_length(images) = 0);
```

## Next Steps

1. Run `npm run test:erpnext` to verify API access
2. Run `npm run fetch:chanel` to generate import file
3. Import JSON to Supabase using one of the methods above
4. Create `pending_products` entries
5. Run AI agents to enrich product data
6. Push enriched products back to ERPNext

## Support

For issues or questions:
- Check `scripts/README-CHANEL-FETCH.md` for detailed documentation
- Review ERPNext API logs at: https://erpnext.mcgrocer.com/app/error-log
- Check Supabase logs in the dashboard
